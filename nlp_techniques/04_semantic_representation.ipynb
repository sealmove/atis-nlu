{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic representation of user utterance\n",
    "\n",
    "We'll process the example dataset utterance:\n",
    "\n",
    "```\n",
    "show me flights from denver to philadelphia on tuesday\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"utterance\": \"show me flights from denver to philadelphia on tuesday\",\n",
      "    \"intent\": \"showFlight\",\n",
      "    \"entities\": {\n",
      "        \"locations\": {\n",
      "            \"from\": \"denver\",\n",
      "            \"to\": \"philadelphia\"\n",
      "        },\n",
      "        \"date\": \"tuesday\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import json\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "utterance = 'show me flights from denver to philadelphia on tuesday'\n",
    "doc = nlp(utterance)\n",
    "\n",
    "def extract_location_entities(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{'POS': 'ADP'}, {'ENT_TYPE': 'GPE'}]\n",
    "    matcher.add('Location', [pattern])\n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for mid, start, end in matches]\n",
    "    locations = {l[0].text: l[1].text for l in spans}\n",
    "    return locations\n",
    "\n",
    "def extract_date_entities(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{'ENT_TYPE': 'DATE'}]\n",
    "    matcher.add('Date', [pattern])\n",
    "    matches = matcher(doc)\n",
    "    spans = [doc[start:end] for mid, start, end in matches]\n",
    "    return spans[0].text if len(spans) > 0 else ''\n",
    "\n",
    "def detect_intent(doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"dobj\":\n",
    "            return token.head.lemma_ + token.lemma_.capitalize()\n",
    "    return ''\n",
    "\n",
    "locations = extract_location_entities(doc)\n",
    "date = extract_date_entities(doc)\n",
    "entities = {'locations': locations, 'date': date}\n",
    "intent = detect_intent(doc)\n",
    "representation = {'utterance': utterance, 'intent': intent, 'entities': entities}\n",
    "\n",
    "print(json.dumps(representation, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a complete semantic representation out of an utterance. This is a machine-readable and usable output. NLP ends here. At this point we can create a practical application which gets as input this json and generates an appropriate response action."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdb829f0a7bb11ab2f1714eb363ce973ffb00a95b9327c3c93f893794322a975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
